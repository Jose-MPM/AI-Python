# Fine-Tuning Comparison: GPT-2 vs LSTM vs Naive Bayes Classifier
## Introduction

Large Language Models (LLMs) have gained prominence in recent years for their outstanding performance in various tasks. One such model, GPT-2, created by OpenAI, was pretrained on a vast amount of internet text in an unsupervised manner. This versatile model can be fine-tuned, a process that involves retraining the preexisting model on a custom dataset tailored to a specific task. Fine-tuning allows the model to consider a broader range of possibilities, leveraging its original training set, and is not confined to specific cases present in a particular dataset.
Objective

The goal of this project is to perform fine-tuning on GPT-2 using a dataset of medical diagnostics. The aim is to obtain a model capable of generating accurate diagnoses based on input descriptions of symptoms. We will compare this fine-tuned GPT-2 model with other models generated using various methods. The objective is to determine the optimal way to leverage the dataset and achieve the best possible performance.
-  Models Comparison
    - Fine-Tuning GPT-2 vs LSTM vs Naive Bayes Classifier
        - [gpt2_diagnosis](gpt2_diagnosis.ipynb): Jupyter Notebook implementing diagnosis using GPT-2.
        - [LSTM](LSTM.ipynb): Jupyter Notebook focusing on Long Short-Term Memory networks.
        - [NaiveBayes](NaiveBayes.ipynb): Jupyter Notebook demonstrating the Naive Bayes classifier.
        - [Presentation(PDF)](Presentacion%20final.pdf): Project Slides
        - [Fine-tuning de GPT-2 para diagn√≥sticos(Article PDF)](Presentacion%20final.pdf): Project Report




Expressions of gratitude üéÅ

    :punch: Share and tell others about this repository üì¢
    :+1: Contact and follow me on GitHub :bowtie:

‚å®Ô∏è with much :purple_heart: by Jose-MPM üòä‚å®Ô∏è